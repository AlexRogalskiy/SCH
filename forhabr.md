# Clickhouse for DWH

(заготовка статьи для хабра. может быть когда-то напишу)

## From ETL to CDC 

Типичная архитектура решений анализа данных предполагает перенос данных из нескольких относительно независимых транзакционных систем, обеспечивающие работу бизнес-процессов компании (OLTP) в единый центр, где эти данные анализируются (OLAP).

Данные из разных источников нужно коррелировать друг с другом, объединять и порождать структуры данных, с которыми будет удобно работать аналитикам.

Такой перенос и трансформации обычно скрывают за аббревиатурами ETL(или ELT), про которые написано немало статей и книг. К настоящему времени сложилось, что ETL/ELT процессы запускаются на периодической основе, чаще всего раз в сутки. В рамках этих процессов данные выкачиваются из OLTP систем и трансформируются тем или иным образом, порождая витрины (Marts) с которыми и работают аналитики.

К настоящему времени наработан достаточно большой инструментарий, который позволяет строить масштабные ETL/ELT решения (например DBT или AirFlow). В рамках таких решений на SQL и процедурном языке пишутся некоторые снипеты, называемые DAG, которые занимается выкачиванием данных и трансформациями, а фреймворк занимается оркестрацией, запуская DAGs через установленные промежутки времени.

Так как данные в OLTP системах постоянно меняются, то такой подход требует многократного скачивания одних и тех-же данных в попытках синхронизовать состояние двух разных баз данных.

Более новый подход призванный решить эту проблему называется CDC - Change data capture. Вместо периодических скачиваний всей таблицы или крупный фрагментов, мы пытаемся найти строки которые были добавлены или изменены, чтобы на их основе восстановить в аналитической базе синхронное с OLTP состояние таблицы.

Попробуем применить такой подход с Clickhouse в качестве аналитической базы данных, написав весь код трансформаций на Clickhouse SQL, без использования каких-то иных пакетов или фреймворков.

### Getting data from MySQL
  - incremental vs full reload
  - mysql() function
  - Kafka
  - MaxWell and Debezium

### Eventual Consistency

Event - событие.
Consistent - https://en.wikipedia.org/wiki/Consistency_(database_systems)

Для OLTP систем очень важно, чтобы данные в разных таблицах были консистентными (соответствующими друг другу). Например, при снятии денег со счета обычно проверяется баланс, делаются проводки, затем меняется баланс.  Без поддержки транзакционности в субд довольно тяжело написать такой алгоритм и не создать опасную ситуацию необоснованного ухода клиентского счета в минус.

В системах для аналитики (OLAP) ситуация иная, нет такой спешки. Мы хотим посмотреть на ситуацию "в общем".  Поэтому мы можем допустить, что данные будут консистентными не в любой отдельно взятый момент времени, а "когда-то".  Например, свежие данные за последний час, или даже за сегодня могут быть не очень консистентными, однако вчерашние данные уже готовы, и там строки в разных таблицах сходятся друг с другом. В некоторых системах может существовать даже месячный цикл (скажем какое-то месячное закрытие счетов-фактур).

Поэтому в DWH (в отличие от OLTP систем) мы допускаем наличие в системе не консистентных данных, однако всегда думаем об их консистентности при составлении конкретного отчета или витрины. 

### **Дубликаты**

- duplicate sources
- Kafka retries
- human mistakes
- на каком уровне дедублицировать?
- дедубликация по контрольщым суммам блоков
- дедубликация обратным просмотром
- last arrival fact problem
- updates and idempotency

В распределенных системах порой случаются ошибки и потери.  Они могут быть вызваны аппаратными или программными сбоями, или же человеческими ошибками.

Мы всегда стоим перед выбором - что делать когда ошибка зафиксирована -  повторить операцию или нет?  Если не повторить, то скорее всего мы потеряем данные, а если повторить, то есть риск создать дубликаты - мы не всегда знаем когда случился “обрыв связи”, и сколько данных было потеряно.

Человеческие ошибки в виде неудачных обновлений ПО, приводящего к потерям данных, тоже весьма вероятны. Исправление таких ошибок зачастую делается перезагрузкой больших фрагментов данных, что тоже может привести к появлению дубликатов. 

Таким образом желательным поведением DWH хранилища было бы игнорирование повторно поступающих данных. 

Американский математик Бенджамин Пирс еще в 19-м веке предложил универсальное понятие идемпотентности как свойство операции не менять состояние объекта при повторении. Скажем сложение с 0 - идемпотентно, а с 1 - нет. Агрегационная функция max даст тот-же самый результат сколько бы вы раз не повторяли вставку в таблицу, а функция sum в общем случае - нет. 

Идемпотентными вставки данных - хороший механизм для создания  стабильной и надежной DWH системы.  Даже частичная идемпотентность, с какими-то дополнительными условиями, позволяет решить эту задачу.

В Clickhouse есть разные методы для реализации идемпотентных вставок.  

1. Проверка контрольных сумм вставляемых блоков.  Вставляемые строки всегда приходят достаточно большими блоками.  Для каждого блока нетрудно посчитать контрольную сумму, сохранить её, а потом проверить при следующей вставке.  Для Replicated таблиц эти контрольные суммы хранятся в ZooKeeper и функционал включен по умолчанию.  Для non-replicated таблиц эту проверку нужно включать в настройках таблицы MergeTree.
2. Проверка по primary key.  При вставке мы можем сделать where id in (select id from same_table). Это гораздо дороже с точки зрения вычислительных возможностей, однако покрывает гораздо больше сценариев повторной загрузки данных.

### Schema-free with JSON type

Передавая данные из одной базы данных в другую мы всегда думаем о будущем.  А в этом будущем схема данных обязательно поменяется и нам что-то с этим придется делать.  Иначе в хорошем случае наш ETL процесс остановится, а в плохом мы получим "неправильные" данные в нашей DWH системе.

Существует два подхода для управления изменениями схемы данных:
1. Schema Registry - изменения схемы фиксируются так-же как и изменения данных, отправляются примерно по тому-же маршруту, хранятся в специальной базе данных, используются всеми заинтересованными. Применяется во всех решениях, основанных на Kafka Connect. Хорошо, дорого, сложно.
2. Schema Inference - поток данных идет в свободном JSON формате, данные по приходу в целевую систему анализируются, при необходимости локальная схема данных обновляется с учетом вновь появившихся данных. Колонки добавляются, типы меняются.

Мы будем использовать вар.2, задействовав новый экспериментальный тип Clickhouse Object('JSON'), предполагая что у нас достаточно проверок, чтобы при серьезных проблемах со схемой сломаться/остановиться с внятной ошибкой, а не работать дальше тихо порождая неверные данные. 

### Event Streams

Потоки событий в нашем мире обычно передаются через Kafka. Продюсеры пишут, консьюмеры - читают, партиции помогают масштабироваться и балансировать нагрузку. Есть немало статей на эту тему - не буду повторяться. Так-же не буду в этой статье углубляться в тему продюсеров. В идеальном случае ваше приложение само пишет транзакционный лог событий в Kafka, откуда они попадают как в OLTP, так и в OLAP базы данных.  В реальной жизни часто приходится получать события через лог транзаций СУБД. Для Postgress такой лог называется WAL, для MySQL - binlog. Есть инструменты, позволяющие читать лог транзакций и писать поток событий в Kafka. Наиболее известный - это Debezium, менее известный MaxWell’s Daemon. Но есть и другие.

Рассмотрим варианты как данные могут попадать из Kafka to Clickhouse.

1. Ваш собственный consumer. Читаем топик, пишем в таблицу. Отличный вариант, но трудоемкий. Надо учесть все особенности многопоточности, кластеризации, вариантов обрывов и не потерять порядок (order) следования событий.
2. Kafka Conect by clickhouse inc - бета, java
3. Altinity Sync - java, debezium, Мне тут не понравилось очень жесткая завязка на схему данных - нужно хранилище схемы, автоматически все изменения схемы в mysql отображаются на структуру kafka потоков и таблиц в Clickhouse. При использовании percona update tool возникают интересные особенности.
4. Clickhouse Kafka Engine - at least once delivery, ограниченный тайминг на выполнение трансформаций. Зато сразу есть вся необходимая кластеризация и многопоточная обработка.

Так как наше решение не должно использовать ничего кроме Clickhouse SQL функций, то наш вариант - 4, несмотря на все его недостатки.  Выглядит он так 

(картинка) Kafka Engine -> MV -> table

### Joins

При проектировании DWH на основе Clickhouse я опираюсь на теорию Ralph Kimball  с денормализоваными широкими таблицами фактов. Таблицы измерений я делаю в виде Clickhouse dictionary, или же применяю LowCardinality (degenerated dimension по Кимбалу).

Чтобы собрать такие широкие таблицы приходится делать Join нескольких таблиц источников. Это тяжелая операция. Особенно для Clickhouse, который изначально умел делать только hash-join, и до сих пор не обладает хорошим оптимизатором запросов.  По возможности такие join лучше делать непосредственно в OLTP системах, развертывая нормализованную группу связанных таблиц в требуемую для анализа широкую таблицу.

Однако такой подход не всегда возможен по двум причинам:

- Команды, занимающиеся OLTP базой не хотят ставить перед собой такую задачу - “вы там как-нибудь сами”
- Для построения широкой таблицы зачастую нужны данные из нескольких различных OLTP систем.

Так что все-таки приходится делать Join в рамках Clickhouse. Даже с различными ручными оптимизациями и трюками такой запрос может занимать несколько минут, что исключает построение “красивого” пайплайна на основе цепочки Materialized Views.

Рассмотрим варианты.

### **AggregatingMergeTree**

Если различные потоки данных не слишком сильно нормализованы и join всех исходных можно сделать по единственному общему ключу (а не цепочки вложенных запросов с join по разным ключам), то можно обойтись совсем без join.

Для этого мы создаем таблицу фактов с Engine=AggregatingMergeTree order by (уникальный ключ). В такую таблицу можно направить потоки событий из разных Kafka topics через разные Materialized Views. Вставка будет быстрая, а слияния (которые по сути заменяют join) будут производиться в бекграунде. Однако чтение данных с финальной агрегацией должно производиться с group by или final. Это несколько замедляет выборку при построении отчетов, но в ряде случаев это можно сделать с удовлетворительной скоростью.

В процессе вставки из отдельного топика мы заполняем только те колонки, которые актуальны для этого типа событий. Остальные колонки будут заполнены значениями по умолчанию или же Null (для Nullable) колонок. В процессе агрегации Nullable and default значения будут (должны) проигнорированы агрегационной функцией, так что в конечном счете строка будет содержать данные во всех колонках.

==spoiler

Несмотря на то, что Nullable типы очень логично работают при агрегации реальных и несуществующих данных, тем не менее я рекомендую избегать Nullable, так как под капотом это еще один столбец с bool значением. Хотя он и занимает на диске не так много места, тем не менее расходы на операции с еще одним столбцом замедляют доступ. Если в ваших данных одно из значений (ноль, -1, пустая строка) никогда не встречается, то правильный выбор агрегационой функции часто позволяет обойтись без Nullable. Например функция max отлично работает не только с числами, но и со строками. Строка с данными всегда больше пустой строки.

Учтите, что Kafka Engine обеспечивает at-least-once гарантию доставки (Delivery Semantics). Поэтому важно правильно выбрать агрегационные функции, так чтобы при вставке обеспечивалась идемпотентность, и повторная операция не портила состояние. 

Лучше всего использовать простые агрегационные функции (SimpleAggregationFunction) такие как max/min - идемпотентные и не хранящие дополнительных данных кроме самого результата агрегации. Для замены операции join этого обычно достаточно. Однако когда помимо простого совмещения данных из разных топиков по общему id делается еще и реальная агрегация, то приходится использовать более сложные агрегационные функции, хранящие дополнительную информацию (такие как argMax/uniq).

Правильный выбор агрегационных функций - основа успешного применения этого метода. Допустим вы хотите накопить в колонке массив значений, и хотите применить функцию groupArray, но она нарушает идемпотентность вставки, а функция groupUniqArray - не нарушает.  Можно делать и более сложные варианты агрегаций. Например, если все-таки нужно посчитать сумму и не нарушить идемпотентность вставки, то можно сохранить список пар (some_uniq_row_id, значение), и посчитать сумму уже во время выборки данных для отчета.

==пример.

### Оркестратор-планировщик

Вышеприведенный способ построения “широкой таблицы” не всегда подходит из-за ограничений по идемпотентности вставки, а так-же в случаях когда схема исходных данных сильно нормализована и содержит группу из большого числа таблиц с несколькими связями по разным ключам. Так-же может оказаться неприемлемы использование final и иных способов окончательной агрегации при построении запросов для отчетов В этом случае приходится строить сложный многоуровневый join и ждать поступления данных из всех таблиц задейстованных в этом join.

Для этого запускается оркестратор (простой скрипт на bash), который выполняет insert-select запросы, внутри которых уже можно делать относительно длинные запросы, включающие join нескольких тяжелых таблиц. Такой запрос должен можно выполнять как периодически с каким-то разумным для задачи интервалом, так и при при достижении некоторых условий - например после того как поступят данные во все связанные таблицы.

В этом методе все приходящие из Kafka данные предварительно сохраняются в Stage таблицах. Это могут быть как EmbeddedRocksDB таблицы, готовые к join=direct, или же обычные MergeTree таблицы, для которых применяется hash join или sort join. В этом случае для каждой вставляемой строки генерируется что-то похожее на sonyflakeid, в виде tuple(now64(),rowNumberInAllBlocks()) по которому можно последовательно идти по таблице выбирая еще не обработанные строки. В чем-то это похоже на оффсет в кафка топике.

### Зависимости, Задержка и Точность.

Предполагается что несколько связанных OLTP систем работают должным образом, предоставляя пользователям некоторую услугу. Данные в разных таблицах и разных базах данных синхронизованы друг с другом самой логикой приложения. (Например информация о продаже не может появиться раньше появления информации о клиенте).

Поэтому с точки зрения DWH системы мы можем предположить что существует некоторый момент времени, когда состояние исходных таблиц позволяет нам сделать join и собрать правильным образом “широкую таблицу фактов”, даже вне рамок полноценной транзакции (известной только самому приложению).

Предполагается, что данные (события) приходят в clickhouse через Kafka с некоторой задержкой, однако в том-же самом порядке как они были созданы приложением. Поэтому контролируя тайстемпы приходящих данных, мы можем опознать ситуацию когда поток данных от какой-то системы остановился, и мы не можем продолжать сборку целевой факт-таблицы.

Для этого строится список зависимых исходных таблиц, для которых мы контролируем время поступления данных и тормозим сборку таблицы фактов.

Предполагается, что поток событий идет равномерно, задержка доставки более-менее постоянна. В этом случае мы можем опираться на таймстемп поступления данных в clickhouse как на свидетельство того, что у нас есть актуальные данные для некоторой таблицы. В случае равной задержки по разным топикам можно считать что минимальный таймстемп из набора зависимых исходных таблиц дает нам достаточно хорошую точку готовности сборки “широкой таблицы”, и мы можем брать в работу данные, которые пришли до этого момента.

Однако системное время на разных серверах может отличаться на секунды и десятки секунд (однако не на десятки минут), а данные от разных источников могут поступать в кафку с разной задержкой. Поэтому лучше дать еще более пессимистичный прогноз, и сделать дополнительную задержку на некоторое количество секунд, в зависимости от конкретных обстоятельств, т.е. конфигурируемо на каждую собираемую “широкую таблицу фактов”.  Чем стабильнее идут исходные данные - тем меньше эта задержка, тем быстрее мы подготовим данные для аналитики. Чем менее предсказуемо ведет себя система доставки событий, тем больше надо ставить эту задержку, чтобы не оказаться в ситуации, что у нас нет необходимых данных для заполнения какого-то столбца широкой таблицы. Безусловно это приведет к более поздней готовности целевых данных - вплоть до часа и более, однако в большинстве случаев это допустимо и гораздо лучше типичного суточного цикла ETL/ELT процессов.

Важно отметить, что само по себе отсутствие данных в каком-то столбце не всегда является проблемой. Данные могут прийти позже (и мы заменим строку), или же единичные проблемы могут быть не важны для конкретных аналитических отчетов. В любом случае все ситуации неполной сборки “широкой таблицы” должны логироваться и мониториться.
